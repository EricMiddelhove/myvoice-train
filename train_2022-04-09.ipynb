{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6184722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.keras.metrics as metrics\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, AveragePooling1D, Conv1D, DenseFeatures, Input, Flatten, GlobalAveragePooling1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab823ec",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "\n",
    "Our dataset contains four directories. Each directory referst to one class of motion to be classified. Each directory contains 395 recordings. \n",
    "\n",
    "---\n",
    "## Recordings\n",
    "Each line of the recording is the average value of the gyro sensor over 10 ms. The whole recording time is 2000ms. Therefore each recording has 200 lines.\n",
    "Each line of the recording contais six different values. They are the acceleration values on all three spatial axes as well as the gyroscope information on all three spatial axes. \n",
    "\n",
    "> The accelerometer values are deliberately not cleaned from earths acceleration.\n",
    "---\n",
    "## Reading the data\n",
    "\n",
    "For the reading of the data we iterate over all directories in the dataset directory, and the over each file in the gesture directory. We then save the data into a python list. We also save the desired classification outcome, at the same time, into another list. That way you will find the classification of the dataset at the same index of the label list as the data in the data list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b346c095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 200, 6)\n",
      "(1584, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"gestures\"\n",
    "\n",
    "gestures = [\"Computer\", \"Elephant\", \"Sorry\", \"No-Sign\"]\n",
    "\n",
    "dataset_values = []\n",
    "dataset_labels = []\n",
    "\n",
    "for gesture in gestures:\n",
    "    for filename in os.listdir(f\"{dataset_dir}/{gesture}\"):\n",
    "        with open(os.path.join(f\"{dataset_dir}/{gesture}\", filename), 'r') as file:\n",
    "            file_data = []\n",
    "\n",
    "            text = file.read()\n",
    "            lines = text.splitlines()\n",
    "\n",
    "            for line in lines:\n",
    "                int_values = []\n",
    "                for value in line.split():\n",
    "                    int_values.append(int(value))\n",
    "\n",
    "                file_data.append(int_values)\n",
    "\n",
    "            dataset_values.append(file_data)\n",
    "\n",
    "            if gesture == \"Computer\":\n",
    "                dataset_labels.append([1, 0, 0, 0])\n",
    "            elif gesture == \"Elephant\":\n",
    "                dataset_labels.append([0, 1, 0, 0])\n",
    "            elif gesture == \"Sorry\":\n",
    "                dataset_labels.append([0, 0, 1, 0])\n",
    "            elif gesture == \"No-Sign\":\n",
    "                dataset_labels.append([0, 0, 0, 1])\n",
    "            else:\n",
    "                print(\"You did not change the labeling of the dataset you blithering idiot\")\n",
    "\n",
    "dataset_values = np.array(dataset_values)\n",
    "dataset_labels = np.array(dataset_labels)\n",
    "\n",
    "print(dataset_values.shape)\n",
    "print(dataset_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da2d42",
   "metadata": {},
   "source": [
    "## Creating the dataset\n",
    "\n",
    "We convert the two lists mentioned above into a tf.Dataset. \n",
    "\n",
    "---\n",
    "\n",
    "First we define SHUFFLE_BUFFER_SIZE and VALIDATION_SIZE. We want to shuffle the complete Dataset, therefore the shuffle buffer has the same size as the dataset itself. We want to have Validationset that is the size of about 30% of the whole Dataset.\n",
    "\n",
    "We create our dataset and shuffle it.\n",
    "\n",
    "For the validation set we take the defined amount from the front of the shuffled dataset. The rest is our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffe0ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 10:26:24.846165: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-20 10:26:24.846290: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE = len(dataset_values)\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = DATASET_SIZE\n",
    "VALIDATION_SIZE = int(DATASET_SIZE * 0.3)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dataset_values, dataset_labels))\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "validation_dataset = dataset.take(VALIDATION_SIZE).batch(BATCH_SIZE)  \n",
    "train_dataset = dataset.skip(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# for values, labels in train_dataset.take(1):\n",
    "    # print(labels.numpy(), values.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f84e9",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c70548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_20 (Conv1D)          (None, 44, 200)           84200     \n",
      "                                                                 \n",
      " global_average_pooling1d_20  (None, 200)              0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 20)                2020      \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 106,404\n",
      "Trainable params: 106,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      " 1/18 [>.............................] - ETA: 4s - loss: 759.2642 - categorical_accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:45:28.485894: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 20ms/step - loss: 221.1909 - categorical_accuracy: 0.7592 - val_loss: 60.6996 - val_categorical_accuracy: 0.9032\n",
      "Epoch 2/20\n",
      " 6/18 [=========>....................] - ETA: 0s - loss: 53.8412 - categorical_accuracy: 0.8568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:45:28.895984: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 14ms/step - loss: 52.5972 - categorical_accuracy: 0.8846 - val_loss: 32.0938 - val_categorical_accuracy: 0.9242\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 29.7313 - categorical_accuracy: 0.9333 - val_loss: 34.7673 - val_categorical_accuracy: 0.9368\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 15.7303 - categorical_accuracy: 0.9531 - val_loss: 3.2768 - val_categorical_accuracy: 0.9789\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 13.5930 - categorical_accuracy: 0.9603 - val_loss: 9.4216 - val_categorical_accuracy: 0.9663\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 8.6021 - categorical_accuracy: 0.9684 - val_loss: 22.1594 - val_categorical_accuracy: 0.9516\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 18.4910 - categorical_accuracy: 0.9495 - val_loss: 13.8623 - val_categorical_accuracy: 0.9642\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 11.4684 - categorical_accuracy: 0.9648 - val_loss: 7.5911 - val_categorical_accuracy: 0.9789\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 17.5076 - categorical_accuracy: 0.9567 - val_loss: 20.9982 - val_categorical_accuracy: 0.9516\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 8.9286 - categorical_accuracy: 0.9702 - val_loss: 12.7463 - val_categorical_accuracy: 0.9621\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 6.1880 - categorical_accuracy: 0.9739 - val_loss: 3.2897 - val_categorical_accuracy: 0.9853\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 8.0504 - categorical_accuracy: 0.9702 - val_loss: 9.2059 - val_categorical_accuracy: 0.9621\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 9.7538 - categorical_accuracy: 0.9639 - val_loss: 2.7945 - val_categorical_accuracy: 0.9768\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 12.3394 - categorical_accuracy: 0.9495 - val_loss: 14.5395 - val_categorical_accuracy: 0.9368\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 5.1084 - categorical_accuracy: 0.9775 - val_loss: 3.8212 - val_categorical_accuracy: 0.9811\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 2.8984 - categorical_accuracy: 0.9829 - val_loss: 3.8628 - val_categorical_accuracy: 0.9853\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 4.1916 - categorical_accuracy: 0.9811 - val_loss: 3.1729 - val_categorical_accuracy: 0.9726\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 3.3114 - categorical_accuracy: 0.9802 - val_loss: 5.7337 - val_categorical_accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.8224 - categorical_accuracy: 0.9739 - val_loss: 1.4126 - val_categorical_accuracy: 0.9874\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 1.6907 - categorical_accuracy: 0.9829 - val_loss: 0.7816 - val_categorical_accuracy: 0.9853\n",
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 s, sys: 2.5 s, total: 7.26 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential([\n",
    "    Conv1D(filters = 200, strides = 3, kernel_size=70, activation='relu', input_shape=(200, 6)),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"CategoricalAccuracy\"])\n",
    "model.build(input_shape=(1584, 200, 6))\n",
    "model.summary()\n",
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n",
    "\n",
    "model.save(\"./model\", save_format='tf')\n",
    "model.save(\"./model.h5\", save_format='h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9124536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:21:32.227073: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-05-20 11:21:32.227086: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-05-20 11:21:32.227157: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: models/0.9900/model\n",
      "2022-05-20 11:21:32.227901: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-05-20 11:21:32.227906: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: models/0.9900/model\n",
      "2022-05-20 11:21:32.229951: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-05-20 11:21:32.255220: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: models/0.9900/model\n",
      "2022-05-20 11:21:32.262338: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 35182 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "324836"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldir = 'models/0.9900/model'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(modeldir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "confusion = tf.confusion_matrix(validation_dataset)\n",
    "\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdec356",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ce06b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 8ms/step - loss: 1.5143 - categorical_accuracy: 0.9768\n",
      "Loss: 1.5142583847045898\n",
      "Accuracy: 0.9768421649932861\n",
      "\n",
      " Confusion Matrix of Validation Set:\n",
      "tf.Tensor(\n",
      "[[120   1   0   1]\n",
      " [  0 126   0   0]\n",
      " [  0   0 115   0]\n",
      " [  2   0   0 110]], shape=(4, 4), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/rc11t0614pd6l8fc81zjgd3h0000gn/T/ipykernel_6206/2063199296.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  labels = np.array(labels)\n",
      "/var/folders/lv/rc11t0614pd6l8fc81zjgd3h0000gn/T/ipykernel_6206/2063199296.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  predictions = np.array(predictions)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "evaluation_results = model.evaluate(validation_dataset)\n",
    "print(\"Loss: {}\".format(evaluation_results[0]))\n",
    "print(\"Accuracy: {}\".format(evaluation_results[1]))\n",
    "\n",
    "## Printing Confusion Matrix\n",
    "labels = []\n",
    "values = []\n",
    "for element in validation_dataset.as_numpy_iterator():\n",
    "  value, label = element\n",
    "  values.append(value)\n",
    "  labels.append(label)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "\n",
    "def get_highest_value(list):\n",
    "  highest_index = 0\n",
    "  for i, val in enumerate(list):\n",
    "    if val > list[highest_index]:\n",
    "      highest_index = i\n",
    "\n",
    "  return highest_index\n",
    "\n",
    "\n",
    "clear_labels = []\n",
    "clear_predicitons = []\n",
    "\n",
    "for elem in values:\n",
    "  predictions.append(model.predict(elem))\n",
    "\n",
    "for j in range(len(labels)):\n",
    "  for i in range(len(labels[j])):\n",
    "    clear_labels.append(get_highest_value(labels[j][i]))\n",
    "    clear_predicitons.append( get_highest_value(predictions[j][i]))\n",
    "\n",
    "\n",
    "# for i, val in enumerate(clear_labels):\n",
    "#   print(f\"label: {clear_labels[i]} pred: {clear_predicitons[i]} eval: {clear_labels[i] == clear_predicitons[i]}\")\n",
    "\n",
    "\n",
    "labels = np.array(labels)  \n",
    "predictions = np.array(predictions)  \n",
    "\n",
    "\n",
    "print(\"\\n Confusion Matrix of Validation Set:\")\n",
    "print(tf.math.confusion_matrix(clear_labels, clear_predicitons))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daad4f9cc88a3975bebf9a8ecbb67b35d04f946432f8642dd57e08037900d3f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myvoice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
